import os
import time
import math
import numpy as np
from typing import List, Tuple

import torch
import torch.nn as nn
import torch.optim as optim

from gym_env_dynamic import PointsEnv
from network import AttentionNet

from valid import build_multi_fixed_envs

import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

import glob


# ========== å¯è°ƒè¶…å‚ ==========
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

GRAPH_TASKS = 100          # æ¯ä¸ªå›¾çš„ä»»åŠ¡ç‚¹æ•°é‡ï¼ˆä½ çš„ env å‚æ•°ï¼‰
MAX_DIST = 3.0             # æ¯ä¸ªå›¾çš„æœ€å¤§é‡Œç¨‹
BATCH_SIZE = 40            # æ¯æ¬¡ update ç”¨ 64 ä¸ªç‹¬ç«‹å›¾/episode
N_EPOCHS = 1800             # ä¸€å…±è®­ç»ƒ 100 ä¸ª epoch
UPDATES_PER_EPOCH = 10   # æ¯ä¸ª epoch è¿›è¡Œ 1000 æ¬¡åå‘ä¼ æ’­ï¼ˆæ¯æ¬¡ 64 å›¾ï¼‰

LR = 1e-4
GAMMA = 1
ENTROPY_COEF = 0.0
MAX_GRAD_NORM = 1
BETA = 0.95                 # EMA baseline å¹³æ»‘
VAL_EPISODES = 100         # éªŒè¯æ—¶çš„æ ·æœ¬æ•°é‡ï¼ˆå¯è°ƒï¼‰
SAVE_DIR = "checkpoint_random_map"
os.makedirs(SAVE_DIR, exist_ok=True)

# ========== å·¥å…·ï¼šå¯¼å…¥æ¨¡å‹ ==========
def find_latest_ckpt(model_dir: str) -> str:
    """åœ¨æŒ‡å®šæ–‡ä»¶å¤¹é‡ŒæŒ‰ä¿®æ”¹æ—¶é—´æ‰¾æœ€æ–°çš„ .pt / .pth"""
    cand = glob.glob(os.path.join(model_dir, "*.pt")) + glob.glob(os.path.join(model_dir, "*.pth"))
    if not cand:
        return None
    cand.sort(key=lambda p: os.path.getmtime(p), reverse=True)
    return cand[0]

def try_resume_from_ckpt(policy: nn.Module,
                         optimizer: optim.Optimizer,
                         baseline: 'ExponentialBaseline',
                         path: str,
                         device: str = "cpu"):   # â† é»˜è®¤ç”¨cpu
    """
    ä» checkpoint æ¢å¤ï¼Œè¿”å› (start_epoch, best_val)ã€‚
    å…ˆå¼ºåˆ¶CPUåŠ è½½ï¼Œé¿å… ROCm ä¸‹ amdsmi åˆå§‹åŒ–é”™è¯¯ï¼›éšåæŠŠæƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿å› deviceã€‚
    """
    print(f"ğŸ” Loading checkpoint (CPU-safe): {path}")
    # æ³¨æ„ï¼šä¸ºäº†èƒ½æ¢å¤ optimizerï¼Œæˆ‘ä»¬è¿™é‡Œä»éœ€è¦ weights_only=False
    ckpt = torch.load(path, map_location="cpu")  # â˜… å…³é”®ï¼šå¼ºåˆ¶åˆ°CPU

    # çº¯ state_dict å…¼å®¹
    if isinstance(ckpt, dict) and ("model" not in ckpt) and \
       ("state_dict" in ckpt or any(isinstance(v, torch.Tensor) for v in ckpt.values())):
        state = ckpt.get("state_dict", ckpt)
        policy.load_state_dict(state)
        policy.to(device)
        print("  âœ… Loaded model weights (state_dict only).")
        return 0, -1e9

    # æ ‡å‡†ä¿å­˜æ ¼å¼
    if "model" in ckpt:
        policy.load_state_dict(ckpt["model"])
        policy.to(device)
        print("  âœ… Loaded model weights.")

    # æ¢å¤ä¼˜åŒ–å™¨ï¼ˆå…ˆåœ¨CPUä¸Šloadï¼Œå†æŠŠstateé‡Œçš„å¼ é‡è¿åˆ°deviceï¼‰
    if "opt" in ckpt:
        try:
            optimizer.load_state_dict(ckpt["opt"])
            # æŠŠä¼˜åŒ–å™¨stateé‡Œçš„å¼ é‡æ¬åˆ°ç›®æ ‡device
            for state in optimizer.state.values():
                for k, v in state.items():
                    if isinstance(v, torch.Tensor):
                        state[k] = v.to(device)
            # è¦†ç›–å­¦ä¹ ç‡ä¸ºå½“å‰è„šæœ¬è®¾å®š
            for pg in optimizer.param_groups:
                pg["lr"] = LR
            print(f"  âœ… Restored optimizer (moved to {device}, lr={LR}).")
        except Exception as e:
            print(f"  âš ï¸ Optimizer state not loaded: {e}")

    # æ¢å¤ EMA baselineï¼ˆæ ‡é‡ï¼‰
    if "baseline" in ckpt:
        try:
            baseline._b = torch.tensor(float(ckpt["baseline"]), dtype=torch.float32, device=device)
            print(f"  âœ… Restored baseline EMA: {baseline.value():.4f}")
        except Exception as e:
            print(f"  âš ï¸ Baseline not restored: {e}")

    start_epoch = int(ckpt.get("epoch", 0))
    best_val = float(ckpt.get("best_val", -1e9))
    print(f"  â–¶ï¸ Resume from epoch={start_epoch}, best_val={best_val:.4f}")
    return start_epoch, best_val


# ===== Resume / Load =====
MODEL_DIR = "model"          # ä½ å­˜æ”¾å·²è®­ç»ƒæ¨¡å‹çš„æ–‡ä»¶å¤¹
RESUME = True                # True=å°è¯•ç»§ç»­è®­ç»ƒï¼›False=ä»å¤´å¼€å§‹
# RESUME_PATH = None           # æ‰‹åŠ¨æŒ‡å®š ckpt è·¯å¾„ï¼ˆä¾‹å¦‚ "model/10-17-best_model_384_5000.pt"ï¼‰
#                              # è‹¥ä¸º None ä¸” RESUME=Trueï¼Œåˆ™è‡ªåŠ¨åœ¨ MODEL_DIR æŒ‰ä¿®æ”¹æ—¶é—´æ‰¾æœ€æ–° *.pt/*.pth

RESUME_PATH = "checkpoints_random_map/best.pt"
#RESUME_PATH = None


# ========== å·¥å…·ï¼šç»˜å›¾ ==========
def ensure_dir(d):
    os.makedirs(d, exist_ok=True)

def now_tag():
    # å½¢å¦‚ 10_19_1935ï¼ˆMM_DD_HHMMï¼‰
    return datetime.now().strftime("%m_%d_%H%M")

def init_result_dir():
    base_dir = os.path.join("result_cur", f"train_{now_tag()}")
    ensure_dir(base_dir)
    print(f"ğŸ“ results will be saved under: {base_dir}")
    return base_dir

def save_excels(base_dir, train_rows, eval_rows):
    """æŒç»­è¦†ç›–å¼ä¿å­˜åˆ°ä¸¤ä»½ Excelã€‚"""
    df_train = pd.DataFrame(train_rows)  # åˆ—: epoch, update, reward, loss
    df_eval  = pd.DataFrame(eval_rows)   # åˆ—: epoch, val_return, eval_time, train_time
    # ä¸¤ä¸ª Excelï¼šä¸€ä¸ªè¯„æµ‹ç»“æœ&è€—æ—¶ï¼Œä¸€ä¸ªè®­ç»ƒ reward/loss
    eval_xlsx  = os.path.join(base_dir, "eval.xlsx")
    train_xlsx = os.path.join(base_dir, "train.xlsx")
    # ä½¿ç”¨ openpyxl å†™ xlsxï¼ˆpandas é»˜è®¤å³å¯ï¼‰
    df_eval.to_excel(eval_xlsx, index=False)
    df_train.to_excel(train_xlsx, index=False)
    return eval_xlsx, train_xlsx

def plot_curves(base_dir, train_rows, eval_rows):
    """è®­ç»ƒç»“æŸï¼ˆæˆ–ä¸­æ–­ï¼‰åç”» 4 å¼ å›¾ï¼šval_returnã€eval_timeã€epochå‡å€¼rewardã€epochå‡å€¼lossã€‚"""
    if len(eval_rows) > 0:
        df_eval = pd.DataFrame(eval_rows)
        # 1) val_return
        plt.figure()
        plt.plot(df_eval["epoch"], df_eval["val_return"])
        plt.xlabel("epoch"); plt.ylabel("val_return (greedy on fixed-100)")
        plt.title("Validation Return over Epochs")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "val_return_curve.png"), dpi=150)
        plt.close()

        # 2) eval_time
        plt.figure()
        plt.plot(df_eval["epoch"], df_eval["eval_time"])
        plt.xlabel("epoch"); plt.ylabel("eval_time (s)")
        plt.title("Evaluation Time over Epochs")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "eval_time_curve.png"), dpi=150)
        plt.close()

    if len(train_rows) > 0:
        df_train = pd.DataFrame(train_rows)  # æ¯ä¸ª update ä¸€è¡Œ
        # èšåˆåˆ° epoch å‡å€¼ï¼Œé¿å…æ›²çº¿å¤ªæŠ–
        df_ep = df_train.groupby("epoch", as_index=False).agg(
            mean_reward=("reward", "mean"),
            mean_loss=("loss", "mean")
        )

        # 3) mean_reward per epoch
        plt.figure()
        plt.plot(df_ep["epoch"], df_ep["mean_reward"])
        plt.xlabel("epoch"); plt.ylabel("mean_reward (per-epoch avg)")
        plt.title("Training Mean Reward per Epoch")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "train_mean_reward_curve.png"), dpi=150)
        plt.close()

        # 4) mean_loss per epoch
        plt.figure()
        plt.plot(df_ep["epoch"], df_ep["mean_loss"])
        plt.xlabel("epoch"); plt.ylabel("mean_loss (per-epoch avg)")
        plt.title("Training Mean Loss per Epoch")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(base_dir, "train_mean_loss_curve.png"), dpi=150)
        plt.close()

# ========== å·¥å…·ï¼šæŒ‡æ•°æ»‘åŠ¨åŸºçº¿ ==========
class ExponentialBaseline:
    def __init__(self, beta: float = 0.8):
        self.beta = beta
        self._b = None

    @torch.no_grad()
    def eval_and_update(self, returns: torch.Tensor) -> torch.Tensor:
        """returns: [B]ï¼Œè¿”å›æ ‡é‡ baselineï¼ˆå¸¸æ•°åŸºçº¿ï¼Œä¸é€æ ·æœ¬ï¼‰"""
        val = returns.mean()  # æ‰¹å‡å€¼
        if self._b is None:
            self._b = val
        else:
            self._b = self.beta * self._b + (1 - self.beta) * val
        return self._b

    def value(self) -> float:
        return float(self._b) if self._b is not None else 0.0

# ========== å•ä¸ª env ä¸Šæ»šåŠ¨ä¸€ä¸ª episodeï¼Œè¿”å›â€œåºåˆ— logp ä¹‹å’Œâ€å’Œâ€œæŠ˜æ‰£å›æŠ¥â€ ==========
def run_one_episode_collect_seq(policy: AttentionNet, env: PointsEnv) -> Tuple[torch.Tensor, torch.Tensor, float]:
    """
    è¿”å›:
      seq_logp_sum: shape [], å½“å‰ç­–ç•¥åœ¨æ•´æ¡è½¨è¿¹ä¸Šçš„ logÏ€ ä¹‹å’Œï¼ˆæ ‡é‡å¼ é‡ï¼‰
      Rt:           shape [], è¯¥æ¡è½¨è¿¹çš„æŠ˜æ‰£å›æŠ¥ï¼ˆæ ‡é‡å¼ é‡ï¼‰
      ep_return:    float, æœªæŠ˜æ‰£æ€»å›æŠ¥ï¼ˆç”¨äºæ‰“å°ç»Ÿè®¡ï¼‰
    è¯´æ˜ï¼š
      - æ•´æ¡è½¨è¿¹æŒ‰â€œé‡‡æ ·â€é€‰åŠ¨ä½œ
      - æ¯ä¸€æ­¥å°† log_prob(action) ç´¯åŠ 
      - æŠ˜æ‰£å›æŠ¥ Rt ç”¨ GAMMA è®¡ç®—
    """
    env.reset(env.world.rewards, env.world.start_depot_index)
    done = False

    step_logps: List[torch.Tensor] = []
    step_rewards: List[float] = []
    ep_return = 0.0

    while not done:
        points, agent_idx, remaining_distance, valid_mask = env.observe()
        # ç»„è£…å¼ é‡ï¼ˆå•æ¡è½¨è¿¹ï¼Œç”¨ batch ç»´åº¦=1ï¼‰
        points_t = torch.tensor(points, dtype=torch.float32, device=DEVICE).unsqueeze(0)            # [1, N, 3]
        agent_t  = torch.tensor([agent_idx], dtype=torch.long, device=DEVICE)                       # [1]
        rem_t    = torch.tensor([[remaining_distance]], dtype=torch.float32, device=DEVICE)         # [1, 1]
        mask_t   = torch.tensor(valid_mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)        # [1, N]

        # å‰å‘å¾—åˆ°åˆ†å¸ƒï¼ˆè®­ç»ƒ=é‡‡æ ·ï¼‰
        dist = policy(points_t, agent_t, rem_t, mask_t.bool())
        action = dist.sample()                                 # [1]
        logp   = dist.log_prob(action).squeeze(0)              # []
        step_logps.append(logp)

        action_int = int(action.item())
        _, reward, done = env.step(action_int)

        step_rewards.append(float(reward))
        ep_return += float(reward)

    # æŠ˜æ‰£å›æŠ¥ï¼ˆreward-to-go çš„é¦–é¡¹ï¼Œç­‰ä»·æ•´æ¡æŠ˜æ‰£å’Œï¼‰
    # Rt = 0.0
    # for r in reversed(step_rewards):
    #     Rt = r + GAMMA * Rt
    # Rt = torch.tensor(Rt, dtype=torch.float32, device=DEVICE)  # []
    Rt = torch.tensor(ep_return, dtype=torch.float32, device=DEVICE)

    # åºåˆ— log æ¦‚ç‡ä¹‹å’Œ
    seq_logp_sum = torch.stack(step_logps).sum()               # []

    return seq_logp_sum, Rt, ep_return

# ========== é‡‡æ ·ä¸€ä¸ª batchï¼ˆ64 æ¡ç‹¬ç«‹å›¾ï¼‰å¹¶è®¡ç®—ä¸€æ¬¡ REINFORCE æŸå¤± ==========
def reinforce_update(policy: AttentionNet, optimizer, baseline: ExponentialBaseline) -> Tuple[float, float]:
    """
    ä¸€æ¬¡æ›´æ–°ï¼šå¯¹ BATCH_SIZE æ¡è½¨è¿¹â€œæŒ‰æ—¶é—´æ­¥å¹¶è¡Œâ€ roll-outï¼Œ
    å¾—åˆ°æ¯æ¡è½¨è¿¹çš„ sum_logp ä¸æ€»å›æŠ¥ï¼Œå†åšåºåˆ—çº§ REINFORCEã€‚
    """
    policy.train()

    # 1) å‡†å¤‡ä¸€æ‰¹ envï¼ˆæ¯ä¸ª env ä¸€å¼ ç‹¬ç«‹å›¾ï¼‰
    envs = [PointsEnv(tasks_number=GRAPH_TASKS, max_distance=MAX_DIST) for _ in range(BATCH_SIZE)]

    # è½¨è¿¹çº§ç´¯è®¡é‡
    sum_logp = torch.zeros(BATCH_SIZE, device=DEVICE)
    ep_return = torch.zeros(BATCH_SIZE, device=DEVICE)
    done = torch.zeros(BATCH_SIZE, dtype=torch.bool, device=DEVICE)

    # 2) åˆå§‹åŒ–è§‚æµ‹ç¼“å­˜
    obs = []
    for e in envs:
        e.reset(e.world.rewards, e.world.start_depot_index)
        obs.append(e.observe())

    # 3) æŒ‰æ—¶é—´æ­¥å¹¶è¡Œ
    step_cap = GRAPH_TASKS + 1  # æœ€å¤šè®¿é—® N æ¬¡ + 1 æ¬¡å›ä»“
    for _ in range(step_cap):
        idxs = [i for i, d in enumerate(done.tolist()) if not d]
        if not idxs:
            break

        # æ‹¼æˆå¤§ batchï¼ˆåªåŒ…å«æœªç»“æŸçš„æ ·æœ¬ï¼‰
        points_t = torch.stack([torch.tensor(obs[i][0], dtype=torch.float32, device=DEVICE) for i in idxs], dim=0)
        agent_t  = torch.tensor([obs[i][1] for i in idxs], dtype=torch.long,    device=DEVICE)
        rem_t    = torch.tensor([[obs[i][2]] for i in idxs], dtype=torch.float32, device=DEVICE)
        mask_t   = torch.stack([torch.tensor(obs[i][3], dtype=torch.bool,  device=DEVICE) for i in idxs], dim=0)

        # ä¸€æ¬¡å‰å‘ï¼ˆGPU çœŸæ­£åƒåˆ°å¤§ batchï¼‰
        dist = policy(points_t, agent_t, rem_t, mask_t)
        actions = dist.sample()                     # [B_active]
        logps   = dist.log_prob(actions)            # [B_active]

        # å›å†™åˆ°å„è‡ª env
        for j, i in enumerate(idxs):
            a = int(actions[j].item())              # è¿™é‡Œ unavoidable çš„ä¸€æ¬¡ .item()
            _, r, d = envs[i].step(a)
            ep_return[i] += float(r)
            sum_logp[i]  += logps[j]
            done[i] = d
            if not d:
                obs[i] = envs[i].observe()

        if done.all():
            break

    # 4) ç»„æˆ returns / seq_logpsï¼ˆOP ç”¨æ€»å¥–åŠ±å³å¯ï¼‰
    returns_t   = ep_return              # [B]
    seq_logps_t = sum_logp               # [B]

    # 5) åŸºçº¿ï¼ˆEMA å¸¸æ•°åŸºçº¿ï¼›æƒ³æ›´ç¨³å¯æ”¹ self-criticalï¼‰
    b   = baseline.eval_and_update(returns_t)   # æ ‡é‡
    adv = returns_t - b
    adv = (adv - adv.mean()) / (adv.std() + 1e-8)

    # 6) åºåˆ—çº§ REINFORCEï¼ˆç»Ÿè®¡ä¸€ä¸‹å‚ä¸ mean çš„å…ƒç´ ä¸ªæ•°ï¼‰
    loss_vec = -(adv.detach() * seq_logps_t)    # [B]
    valid = torch.isfinite(loss_vec)
    used = int(valid.sum().item())
    # å¯é€‰è°ƒè¯•ï¼š
    # print(f"[debug] loss mean over {used}/{loss_vec.numel()} elements")

    reinforce_loss = loss_vec[valid].mean()
    loss = reinforce_loss

    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    nn.utils.clip_grad_norm_(policy.parameters(), MAX_GRAD_NORM)
    optimizer.step()

    return float(returns_t.mean().item()), float(loss.item())


# ===== åŸºäº GPU çš„å¹¶è¡Œè´ªå¿ƒè¯„æµ‹ï¼ˆå›ºå®šå›¾é›†ï¼‰=====
@torch.inference_mode()    # æ¯” no_grad æ›´å¿«ï¼Œå¯ç”¨ä¸€äº›æ¨ç†ä¼˜åŒ–
def evaluate_greedy_fixed_gpu(policy: AttentionNet, eval_envs, device=DEVICE) -> float:
    """
    åœ¨ä¼ å…¥çš„ eval_envsï¼ˆå›ºå®šçš„100å¼ æˆ–ä»»æ„å¼ å›¾ï¼‰ä¸Šåšè´ªå¿ƒè¯„æµ‹ã€‚
    å…³é”®ï¼šæŒ‰æ—¶é—´æ­¥æŠŠæ‰€æœ‰â€œæœªç»“æŸâ€çš„ç¯å¢ƒæ‰“åŒ…æˆä¸€ä¸ªå¤§ batch å‰å‘ï¼Œä»è€Œç”¨æ»¡ GPUã€‚
    """
    policy.eval()

    M = len(eval_envs)
    # 1) æ¯ä¸ªç¯å¢ƒ reset å›â€œå›ºå®šçš„é‚£å¼ å›¾â€
    for env in eval_envs:
        env.reset(env.world.rewards, env.world.start_depot_index)

    # 2) è½¨è¿¹ç´¯è®¡é‡
    ep_return = torch.zeros(M, device=device)
    done = torch.zeros(M, dtype=torch.bool, device=device)

    # 3) é¢„å–è§‚æµ‹ç¼“å­˜ï¼ˆåªåœ¨éœ€è¦æ—¶åˆ·æ–°ï¼‰
    obs = [env.observe() for env in eval_envs]

    # 4) æŒ‰æ—¶é—´æ­¥å¹¶è¡Œï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
    step_cap = GRAPH_TASKS + 1   # æœ€å¤šè®¿é—® N æ¬¡ + 1 æ¬¡å›ä»“
    for _ in range(step_cap):
        # åªå–æœªç»“æŸçš„æ ·æœ¬ç´¢å¼•
        active = (~done).nonzero(as_tuple=False).view(-1)
        if active.numel() == 0:
            break

        # â€”â€” æ„é€ è¿™ä¸€æ—¶é—´æ­¥çš„å¤§ batch â€”â€” #
        # points: [B_act, N, 2æˆ–d]ï¼› agent: [B_act]ï¼› rem: [B_act, 1]ï¼› mask: [B_act, N]
        points_t = torch.stack([
            torch.as_tensor(obs[i][0], dtype=torch.float32, device=device) for i in active.tolist()
        ], dim=0)
        agent_t = torch.as_tensor([obs[i][1] for i in active.tolist()],
                                  dtype=torch.long, device=device)
        rem_t   = torch.as_tensor([[obs[i][2]] for i in active.tolist()],
                                  dtype=torch.float32, device=device)
        mask_t  = torch.stack([
            torch.as_tensor(obs[i][3], dtype=torch.bool, device=device) for i in active.tolist()
        ], dim=0)

        # â€”â€” å•æ¬¡å‰å‘åƒä¸‹æ•´ä¸ª batch â€”â€” #
        dist = policy(points_t, agent_t, rem_t, mask_t)
        # è´ªå¿ƒåŠ¨ä½œï¼ˆä¸ç”¨ sampleï¼Œç›´æ¥ argmaxï¼‰
        actions = torch.argmax(dist.probs, dim=-1)   # [B_act]

        # â€”â€” æŠŠåŠ¨ä½œå†™å›å„è‡ªçš„ envï¼Œå¹¶ç´¯ç§¯å›æŠ¥ â€”â€” #
        # æ³¨æ„ï¼šenv æ˜¯ Python å®ç°ï¼Œæ— æ³•çœŸæ­£ vectorized stepï¼Œåªèƒ½é€ä¸ªå†™å›ï¼›
        # ä½†å‰å‘å·²ç»æ‰¹å¤„ç†äº†ï¼ŒGPU åƒæ»¡äº†ï¼Œè¿™é‡Œåªæœ‰å¾ˆè½»çš„ Python å¾ªç¯å¼€é”€ã€‚
        for j, i in enumerate(active.tolist()):
            a = int(actions[j].item())
            _, r, d = eval_envs[i].step(a)
            ep_return[i] += float(r)
            done[i] = d
            if not d:
                obs[i] = eval_envs[i].observe()

        if done.all():
            break

    # 5) è¿”å›å¹³å‡å›æŠ¥ï¼ˆæˆ–ä½ è¦çš„ä»»ä½•ç»Ÿè®¡é‡ï¼‰
    return float(ep_return.mean().item())


# ========== è´ªå¿ƒè¯„ä¼° ==========
#@torch.no_grad()
# def evaluate_greedy(policy: AttentionNet, n_eval: int = VAL_EPISODES) -> float:
#     """
#     ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆargmaxï¼‰åœ¨ n_eval ä¸ªç‹¬ç«‹å›¾ä¸Šè¯„ä¼°å¹³å‡â€œæœªæŠ˜æ‰£æ€»å›æŠ¥â€ã€‚
#     """
#     policy.eval()
#     rets = []
#     for _ in range(n_eval):
#         env = PointsEnv(tasks_number=GRAPH_TASKS, max_distance=MAX_DIST)
#         env.reset(env.world.rewards, env.world.start_depot_index)
#         done = False
#         ep_ret = 0.0
#         while not done:
#             points, agent_idx, remaining_distance, valid_mask = env.observe()
#             points_t = torch.tensor(points, dtype=torch.float32, device=DEVICE).unsqueeze(0)
#             agent_t  = torch.tensor([agent_idx], dtype=torch.long, device=DEVICE)
#             rem_t    = torch.tensor([[remaining_distance]], dtype=torch.float32, device=DEVICE)
#             mask_t   = torch.tensor(valid_mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)
#             dist = policy(points_t, agent_t, rem_t, mask_t.bool())
#             action = torch.argmax(dist.probs, dim=-1).item()
#             _, r, done = env.step(int(action))
#             ep_ret += float(r)
#         rets.append(ep_ret)
#     return float(np.mean(rets))

# # ===== ç”¨å›ºå®š 100 å¼ å›¾åšè´ªå¿ƒè¯„æµ‹ =====
# @torch.no_grad()
# def evaluate_greedy_fixed(policy: AttentionNet, eval_envs) -> float:
#     """
#     åœ¨ä¼ å…¥çš„å›ºå®šè¯„æµ‹ç¯å¢ƒåˆ—è¡¨ eval_envs ä¸Šåšè´ªå¿ƒè¯„æµ‹ã€‚
#     æ¯æ¬¡è¯„æµ‹éƒ½ä¼šæŠŠå„è‡ªç¯å¢ƒ reset å›åˆ°åŒä¸€å¼ å›¾ï¼ˆåŒå¥–åŠ±&èµ·ç‚¹ï¼‰ã€‚
#     """
#     policy.eval()
#     rets = []
#     for env in eval_envs:
#         # ç¡®ä¿å›åˆ°åŒä¸€å¼ å›¾ï¼šä½¿ç”¨è¯¥ env è‡ªå·±çš„ world å‚æ•°é‡ç½®
#         env.reset(env.world.rewards, env.world.start_depot_index)
#
#         done = False
#         ep_ret = 0.0
#         while not done:
#             points, agent_idx, remaining_distance, valid_mask = env.observe()
#
#             points_t = torch.tensor(points, dtype=torch.float32, device=DEVICE).unsqueeze(0)
#             agent_t  = torch.tensor([agent_idx], dtype=torch.long, device=DEVICE)
#             rem_t    = torch.tensor([[remaining_distance]], dtype=torch.float32, device=DEVICE)
#             mask_t   = torch.tensor(valid_mask, dtype=torch.float32, device=DEVICE).unsqueeze(0)
#
#             dist = policy(points_t, agent_t, rem_t, mask_t.bool())
#             action = torch.argmax(dist.probs, dim=-1).item()
#
#             _, r, done = env.step(int(action))
#             ep_ret += float(r)
#
#         rets.append(ep_ret)
#     return float(np.mean(rets))


# ========== ä¸»è®­ç»ƒ ==========
# def main():
#     torch.backends.cudnn.deterministic = True
#
#     policy = AttentionNet().to(DEVICE)
#     optimizer = optim.Adam(policy.parameters(), lr=LR)
#     baseline = ExponentialBaseline(beta=BETA)
#
#     best_val = -1e9
#     ckpt_best = os.path.join(SAVE_DIR, "best.pt")
#
#     #100eval_envs
#     eval_envs = build_fixed_eval_envs(n_envs=100, tasks=GRAPH_TASKS, max_dist=MAX_DIST, seed=2025)
#
#     val_results = []
#     val_times = []
#
#     for epoch in range(1, N_EPOCHS + 1):
#         t0 = time.time()
#         loss_meter = []
#         cost_meter = []
#
#         # â€”â€” æ¯ä¸ª epoch åš 1000 æ¬¡æ›´æ–°ï¼Œæ¯æ¬¡åŸºäº 64 ä¸ªç‹¬ç«‹å›¾ â€”â€” #
#         for u in range(1, UPDATES_PER_EPOCH + 1):
#             mean_reward, loss_val = reinforce_update(policy, optimizer, baseline)
#             loss_meter.append(loss_val)
#             cost_meter.append(mean_reward)
#
#             # ä¹Ÿå¯ä»¥æ¯éš”ä¸€å®šæ­¥æ•°æ‰“å°ä¸€æ¬¡
#             if u % 1 == 0:
#                 print(f"update{u:4d}/{UPDATES_PER_EPOCH}, reward={mean_reward}  loss={loss_val:.4f}")
#
#         # éªŒè¯ï¼ˆè´ªå¿ƒï¼‰
#         t_dur = time.time() - t0
#
#         t0_v = time.time()
#         #val_ret = evaluate_greedy(policy, n_eval=VAL_EPISODES)
#         val_ret = evaluate_greedy_fixed(policy, eval_envs)
#         v_dur = time.time() - t0_v
#
#         val_results.append(val_ret)
#         val_times.append(v_dur)
#
#
#         print(
#             f"[Epoch {epoch:03d}/{N_EPOCHS}] "
#               f"updates={UPDATES_PER_EPOCH}  "
#               f"train_loss={np.mean(loss_meter):.4f}  "
#               f"train_reward(neg_ret)={np.mean(cost_meter):.2f}  "
#               f"val_return(greedy)={val_ret:.2f}  "
#               f"baseline={baseline.value():.2f}  "
#               )
#
#         print(
#               f"train_one_epoch_time={t_dur:.2f}   "
#               f"validate_one_epoch_time={v_dur:.2f}"
#         )
#
#
#         # ä¿å­˜æœ€å¥½
#         if val_ret > best_val:
#             best_val = val_ret
#             torch.save({"model": policy.state_dict(),
#                         "opt": optimizer.state_dict(),
#                         "baseline": baseline.value(),
#                         "epoch": epoch}, ckpt_best)
#             print(f"  âœ… Saved best to {ckpt_best} (val_return={best_val:.2f})")
#
#         print(f"best_val={best_val:.2f}")
#
#     # æœŸæœ«å†å­˜ä¸€ä»½
#     ckpt_last = os.path.join(SAVE_DIR, "last.pt")
#     torch.save({"model": policy.state_dict(),
#                 "opt": optimizer.state_dict(),
#                 "baseline": baseline.value(),
#                 "epoch": N_EPOCHS}, ckpt_last)
#     print(f"Training done. Last saved to {ckpt_last}")
#     print(f"best_val={best_val:.2f}")

def main():
    torch.backends.cudnn.deterministic = True

    # ============ ç»“æœç›®å½• ============
    BASE_DIR = init_result_dir()
    ckpt_best = os.path.join(SAVE_DIR, "best.pt")  # ä½ åŸæœ‰ç›®å½•ä¹Ÿä¿ç•™
    ckpt_last = os.path.join(SAVE_DIR, "last.pt")

    # ============ åˆå§‹åŒ–ç»„ä»¶ ============
    policy = AttentionNet().to(DEVICE)
    optimizer = optim.Adam(policy.parameters(), lr=LR)
    baseline = ExponentialBaseline(beta=BETA)

    # å›ºå®š 100 å¼ å›¾ï¼ˆä½ å·²æœ‰ï¼‰
    eval_envs = build_multi_fixed_envs(n_envs=300, tasks=GRAPH_TASKS, max_dist=MAX_DIST, seed=2025)

    # â€”â€” Resume / ç»§ç»­è®­ç»ƒ â€”â€” #
    start_epoch = 0
    best_val = -1e9
    if RESUME:
        load_path = RESUME_PATH if RESUME_PATH is not None else find_latest_ckpt(MODEL_DIR)
        if load_path is not None and os.path.exists(load_path):
            start_epoch, best_val = try_resume_from_ckpt(policy, optimizer, baseline, load_path, device=DEVICE)
        else:
            print(f"â„¹ï¸ No checkpoint found to resume (looked at {RESUME_PATH or MODEL_DIR}). Start fresh.")

    # ============ æ—¥å¿—ç¼“å­˜ï¼ˆå†…å­˜é‡Œï¼‰ ============
    # è®­ç»ƒåˆ†å¸ƒï¼šæ¯ä¸ª update è®°ä¸€è¡Œ
    train_rows = []  # {epoch, update, reward, loss}
    # è¯„æµ‹ï¼šæ¯ä¸ª epoch è®°ä¸€è¡Œ
    eval_rows  = []  # {epoch, val_return, eval_time, train_time}

    try:
        for epoch in range(1, N_EPOCHS + 1):
            t0_train = time.time()
            loss_meter = []
            reward_meter = []

            # â€”â€” æ¯ä¸ª epoch åš 1000 æ¬¡æ›´æ–°ï¼Œæ¯æ¬¡åŸºäº 64 ä¸ªç‹¬ç«‹å›¾ â€”â€” #
            for u in range(1, UPDATES_PER_EPOCH + 1):
                mean_reward, loss_val = reinforce_update(policy, optimizer, baseline)
                loss_meter.append(loss_val)
                reward_meter.append(mean_reward)

                # è®­ç»ƒåˆ†å¸ƒï¼šé€ update è®°å½•ä¸€è¡Œ
                train_rows.append({
                    "epoch": epoch,
                    "update": u,
                    "reward": float(mean_reward),
                    "loss": float(loss_val),
                })

                # æ§åˆ¶å°æ‰“å°é¢‘ç‡ï¼ˆå¯è°ƒï¼‰
                if u % 1 == 0:
                    print(f"update {u:4d}/{UPDATES_PER_EPOCH}, reward={mean_reward:.4f}  loss={loss_val:.4f}")

            train_dur = time.time() - t0_train

            # â€”â€” è¯„æµ‹ï¼ˆå›ºå®š100å›¾ï¼Œè´ªå¿ƒï¼‰â€”â€”
            t0_eval = time.time()
            #val_ret = evaluate_greedy_fixed(policy, eval_envs)
            val_ret = evaluate_greedy_fixed_gpu(policy, eval_envs, device=DEVICE)
            eval_dur = time.time() - t0_eval

            # è¯„æµ‹æ—¥å¿—ï¼šæ¯ä¸ª epoch ä¸€è¡Œ
            eval_rows.append({
                "epoch": epoch,
                "val_return": float(val_ret),
                "eval_time": float(eval_dur),
                "train_time": float(train_dur),
            })

            # â€”â€” æ§åˆ¶å°ç»Ÿè®¡ â€”â€”
            print(
                f"[Epoch {epoch:03d}/{N_EPOCHS}] "
                f"updates={UPDATES_PER_EPOCH}  "
                f"train_loss(avg)={np.mean(loss_meter):.4f}  "
                f"train_reward(avg)={np.mean(reward_meter):.4f}  "
                f"val_return(greedy)={val_ret:.4f}  "
                f"baseline(ema)={baseline.value():.4f}  "
            )

            print(f"train_time={train_dur:.2f}s  eval_time={eval_dur:.2f}s")

            # â€”â€” ä¿å­˜æœ€å¥½ï¼ˆæŒ‰ val_ret è¶Šå¤§è¶Šå¥½ï¼‰â€”â€”
            if val_ret > best_val:
                best_val = val_ret
                torch.save({
                    "model": policy.state_dict(),
                    "opt": optimizer.state_dict(),
                    "baseline": baseline.value(),
                    "epoch": epoch,
                    "best_val": best_val,
                }, ckpt_best)
                print(f"  âœ… Saved best to {ckpt_best} (val_return={best_val:.4f})")

            print(f"best_val={best_val:.4f}")

            # â€”â€” æ¯ä¸ª epoch ç»“æŸå°±è½ç›˜ä¸€æ¬¡ Excelï¼ŒæŠ—ä¸­æ–­ â€”â€”
            save_excels(BASE_DIR, train_rows, eval_rows)

    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user (KeyboardInterrupt). Saving partial results...")

    finally:
        # â€”â€” æœŸæœ«ï¼ˆæˆ–ä¸­æ–­ï¼‰ä¿å­˜ last æ¨¡å‹ â€”â€”
        torch.save({
            "model": policy.state_dict(),
            "opt": optimizer.state_dict(),
            "baseline": baseline.value(),
            "epoch": min(len(eval_rows), N_EPOCHS),
            "best_val": best_val,
        }, ckpt_last)
        print(f"ğŸ“¦ Last checkpoint saved to {ckpt_last}")

        # â€”â€” ç¡®ä¿æŠŠ Excel è½ç›˜ â€”â€”
        eval_xlsx, train_xlsx = save_excels(BASE_DIR, train_rows, eval_rows)
        print(f"ğŸ“‘ Excel saved: \n  - {eval_xlsx}\n  - {train_xlsx}")

        # â€”â€” ç”»æ›²çº¿ PNG â€”â€”
        plot_curves(BASE_DIR, train_rows, eval_rows)
        print(f"ğŸ“ˆ Curves saved under: {BASE_DIR}")


if __name__ == "__main__":
    main()
